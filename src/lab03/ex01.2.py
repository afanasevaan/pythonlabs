def normalize(text: str, *, casefold: bool = True, yo2e: bool = True):
    s = text
    if casefold:
        s = s.casefold()
    if yo2e:
        s = s.replace('—ë', '–µ').replace('–Å', '–ï')
    s = s.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
    s = s.strip()
    while '  ' in s:
        s = s.replace('  ', ' ')
    return s

import re # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π

def tokenize(text: str):
    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ "—Å–ª–æ–≤", —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ –±—É–∫–≤, —Ü–∏—Ñ—Ä, –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è –∏ –¥–µ—Ñ–∏—Å–æ–≤ –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞
    tokens = re.findall(r'\w+(?:-\w+)*', text) 
    return tokens

example_tokenize1 = "emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"
example_tokenize2 = "2025 –≥–æ–¥"
example_tokenize3 = "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"
example_tokenize4 = "hello,world!!!"

res_tokenize1 = tokenize(example_tokenize1)
res_tokenize2 = tokenize(example_tokenize2)
res_tokenize3 = tokenize(example_tokenize3)
res_tokenize4 = tokenize(example_tokenize4)

print(res_tokenize1)
print(res_tokenize2)
print(res_tokenize3)
print(res_tokenize4)
